{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WchXK6QRkprb",
        "outputId": "b98049b7-2148-4377-dff2-acce4f692dd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textdistance in /usr/local/lib/python3.12/dist-packages (4.6.3)\n",
            "Requirement already satisfied: metaphone in /usr/local/lib/python3.12/dist-packages (0.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install textdistance\n",
        "!pip install metaphone\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import difflib\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import textdistance\n",
        "from metaphone import doublemetaphone"
      ],
      "metadata": {
        "id": "hkdrHyZNli0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Corpus loading and preprocessing\n",
        "# -----------------------------\n",
        "def load_corpus(file_path=\"/content/corpus.txt\"):\n",
        "    \"\"\"\n",
        "    Load corpus text from the given path.\n",
        "    \"\"\"\n",
        "\n",
        "    p = Path(file_path)\n",
        "    return p.read_text(encoding=\"utf-8\")  # Unicode Transformation Format – 8-bit\n",
        "\n",
        "\n",
        "# Load text and basic tokenization\n",
        "raw_text = load_corpus()\n",
        "tokens = re.findall(r\"\\w+\", raw_text.lower())  # tokenization words with convert the string in lower case then apply re.findall with \\w+ syntax\n"
      ],
      "metadata": {
        "id": "-2q7ZTbUnbQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this above cell I create a function to lode and extract the corpus data set that is used in this project.Here use path that is import from path lab and read and extract the text using utf-8 encoding.After getting the raw text using re library tokenize into words with select \\w+ texts and lower the all words."
      ],
      "metadata": {
        "id": "doT4OVh0FlCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0okS96u4ka0",
        "outputId": "bf8bd0bc-7f8e-4112-8390-9bc596824668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Filter words: alphabetic, not a stopword, length > 2\n",
        "filtered = [w for w in tokens if w.isalpha() and w not in stop_words and len(w) > 2]\n",
        "vocabulary = set(filtered)\n",
        "word_freq = Counter(filtered)\n",
        "total_tokens = sum(word_freq.values())\n",
        "\n",
        "# Probabilities (if needed later)\n",
        "probs = {w: freq / total_tokens for w, freq in word_freq.items()}"
      ],
      "metadata": {
        "id": "eDs0sDgA4yfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the tokens remove the stopwords and filtered them. In vocabulary add the all uniqque filttered words and using counter create a dictaniory where key is the word and value is the count of this word."
      ],
      "metadata": {
        "id": "BvathIopGd-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Bigram model builder\n",
        "# -----------------------------\n",
        "def build_bigram_counts(token_list):\n",
        "    \"\"\"\n",
        "    Build bigram counts: dict[word] -> Counter(next_word -> count)\n",
        "    \"\"\"\n",
        "    bigrams = defaultdict(Counter)\n",
        "    for a, b in zip(token_list[:-1], token_list[1:]):\n",
        "        bigrams[a][b] += 1\n",
        "    return bigrams\n",
        "\n",
        "bigram_counts = build_bigram_counts(filtered)   # Use token"
      ],
      "metadata": {
        "id": "DmHzBtmw8elj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "build a bigram model where check the next word corrosponding to the previous word and how many times the next word appear corrospond to the previous word."
      ],
      "metadata": {
        "id": "RMwX9U_kHIGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Phonetic candidate finder\n",
        "# -----------------------------\n",
        "def phonetic_candidates(word, vocab):\n",
        "    \"\"\"\n",
        "    Return vocab words whose Double Metaphone codes match the given word's codes.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        target_codes = doublemetaphone(word)\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "    matches = []\n",
        "    for v in vocab:\n",
        "        try:\n",
        "            v_codes = doublemetaphone(v)\n",
        "        except Exception:\n",
        "            continue\n",
        "        # consider match if primary codes match or secondary codes match\n",
        "        if v_codes[0] == target_codes[0] and v_codes[0] != \"\":\n",
        "            matches.append(v)\n",
        "        elif target_codes[1] and v_codes[1] == target_codes[1]:\n",
        "            matches.append(v)\n",
        "    return matches"
      ],
      "metadata": {
        "id": "7UMTH7_p-e1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using doublemetaphone fn check the phonetic matchin words corrospond to the word from the all vocab."
      ],
      "metadata": {
        "id": "7WP906d9Hh5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_spelling(input_text):\n",
        "    \"\"\"\n",
        "    Spell-check / suggestions (Combined output only)\n",
        "    Uses 3 techniques with weighted combination:\n",
        "      - difflib (0.3)\n",
        "      - phonetic (0.3)\n",
        "      - textdistance.jaro_winkler (0.4)\n",
        "    Returns top 3 best-matched words overall.\n",
        "    \"\"\"\n",
        "\n",
        "    global vocabulary, word_freq, stop_words\n",
        "\n",
        "    # sanity checks\n",
        "    if 'vocabulary' not in globals():\n",
        "        raise RuntimeError(\"Global 'vocabulary' not found\")\n",
        "    if 'word_freq' not in globals():\n",
        "        raise RuntimeError(\"Global 'word_freq' not found\")\n",
        "    if 'stop_words' not in globals():\n",
        "        stop_words = set()\n",
        "\n",
        "    words = re.findall(r\"\\w+\", input_text.lower())   # basically here convert the word into lower form with tokens\n",
        "    if not words:\n",
        "        return \"No input provided\"\n",
        "\n",
        "    jaro_fn = textdistance.jaro_winkler.normalized_similarity  # use jaro winkler fn from textdistance\n",
        "\n",
        "    def combined_topk(w, k=3, difflib_seed_n=10, freq_seed_n=50):\n",
        "        \"\"\"\n",
        "        Weighted combination:\n",
        "          score = 0.3 * difflib + 0.3 * phonetic + 0.4 * jaro_winkler\n",
        "        Returns top-k best candidates.\n",
        "        \"\"\"\n",
        "        cand_set = set()\n",
        "\n",
        "        # difflib seeds\n",
        "        try:\n",
        "            cand_set.update(difflib.get_close_matches(w, list(vocabulary), n=difflib_seed_n, cutoff=0.2))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # phonetic seeds\n",
        "        try:\n",
        "            cand_set.update(phonetic_candidates(w, vocabulary))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # top frequency words\n",
        "        try:\n",
        "            cand_set.update([cand for cand, _ in list(word_freq.most_common(freq_seed_n))])\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        if not cand_set:\n",
        "            cand_set = set(word_freq.keys())\n",
        "\n",
        "        scored = []\n",
        "        for cand in cand_set:\n",
        "            # difflib ratio\n",
        "            try:\n",
        "                s_d = difflib.SequenceMatcher(None, w, cand).ratio()\n",
        "            except Exception:\n",
        "                s_d = 0.0\n",
        "\n",
        "            # phonetic indicator\n",
        "            try:\n",
        "                s_p = 1.0 if cand in phonetic_candidates(w, vocabulary) else 0.0\n",
        "            except Exception:\n",
        "                s_p = 0.0\n",
        "\n",
        "            # jaro-winkler\n",
        "            try:\n",
        "                s_j = jaro_fn(w, cand)\n",
        "            except Exception:\n",
        "                s_j = 0.0\n",
        "\n",
        "            score = 0.3 * s_d + 0.3 * s_p + 0.4 * s_j\n",
        "            scored.append((cand, score, word_freq.get(cand, 0)))\n",
        "\n",
        "        # sort by score desc, then frequency desc\n",
        "        scored.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
        "        return [cand for cand, s, f in scored[:k] if s > 0.0]\n",
        "\n",
        "    # single word case → just return top 3 combined matches\n",
        "    if len(words) == 1:\n",
        "        w = words[0]\n",
        "        if w in vocabulary or w in stop_words:\n",
        "            return \"Correct\"\n",
        "        return combined_topk(w, k=3)\n",
        "\n",
        "    # sentence case → return combined correction suggestions for each word\n",
        "    output = []\n",
        "    for w in words:\n",
        "        if w in stop_words or w in vocabulary:\n",
        "            output.append(w)\n",
        "        else:\n",
        "            output.append(combined_topk(w, k=3))\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "_ci228HhuTne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(input_text):\n",
        "    \"\"\"\n",
        "    Look at the last word in input_text and return up to 3 top next-word predictions\n",
        "    from the bigram model. Interaction:\n",
        "      - If last word is known: show top-3 predictions and let the user\n",
        "        confirm one (by number or Enter for top) or type a custom word to add.\n",
        "        Confirmation/custom word increments the corresponding counts by +1.\n",
        "      - If last word is unseen: prompt the user to add a predicted next word (as before).\n",
        "    \"\"\"\n",
        "    global bigram_counts, word_freq, vocabulary, probs\n",
        "\n",
        "    words = re.findall(r\"\\w+\", input_text.lower())\n",
        "    if not words:\n",
        "        return \"No input provided\"\n",
        "\n",
        "    last = words[-1]\n",
        "\n",
        "    # Ensure data structures exist (safety)\n",
        "    if 'bigram_counts' not in globals():\n",
        "        bigram_counts = defaultdict(Counter)\n",
        "    if 'word_freq' not in globals():\n",
        "        word_freq = Counter()\n",
        "    if 'vocabulary' not in globals():\n",
        "        vocabulary = set()\n",
        "    if 'probs' not in globals():\n",
        "        probs = {}\n",
        "\n",
        "    # --- Known word branch: show predictions and allow confirm/add ---\n",
        "    if last in bigram_counts and bigram_counts[last]:\n",
        "        preds = [w for w, _ in bigram_counts[last].most_common(3)]\n",
        "        # Display predictions and prompt the user for confirmation or custom entry\n",
        "        print(f\"Top predictions for '{last}':\")\n",
        "        for i, p in enumerate(preds, start=1):\n",
        "            print(f\"  {i}. {p}\")\n",
        "        prompt = (\n",
        "            \"If correct, press Enter to accept the top prediction,\\n\"\n",
        "            \"or type 1/2/3 to confirm a specific prediction,\\n\"\n",
        "            \"or type your own word to add it as the predicted next word:\\n> \"\n",
        "        )\n",
        "        user_choice = input(prompt).strip()\n",
        "\n",
        "        # If user pressed Enter, accept top prediction\n",
        "        if user_choice == \"\":\n",
        "            chosen = preds[0]\n",
        "        else:\n",
        "            # If user typed a number 1-3 and that index exists, choose that\n",
        "            if user_choice.isdigit():\n",
        "                idx = int(user_choice)\n",
        "                if 1 <= idx <= len(preds):\n",
        "                    chosen = preds[idx - 1]\n",
        "                else:\n",
        "                    # invalid number -> treat as custom word candidate below\n",
        "                    chosen = None\n",
        "            else:\n",
        "                chosen = None\n",
        "\n",
        "        # If chosen is None, try to parse user_choice as a custom word\n",
        "        if chosen is None:\n",
        "            cand = re.findall(r\"\\w+\", user_choice.lower())\n",
        "            if not cand:\n",
        "                # user typed invalid input — do nothing, return preds as before\n",
        "                return preds\n",
        "            chosen = cand[0]  # the custom word user wants\n",
        "\n",
        "        # Update bigram_counts and global counts for the chosen word (confirm or custom)\n",
        "        bigram_counts[last][chosen] += 1\n",
        "        word_freq[chosen] += 1\n",
        "        vocabulary.add(chosen)\n",
        "\n",
        "        # Recompute probabilities\n",
        "        total_tokens = sum(word_freq.values()) if word_freq else 0\n",
        "        if total_tokens > 0:\n",
        "            probs = {w: f / total_tokens for w, f in word_freq.items()}\n",
        "\n",
        "        # Return updated top-3 predictions for last (so caller sees the result)\n",
        "        updated_preds = [w for w, _ in bigram_counts[last].most_common(3)]\n",
        "        return {\n",
        "            \"message\": f\"Confirmed/added prediction: ('{last}' -> '{chosen}')\",\n",
        "            \"top_predictions_for_last\": updated_preds,\n",
        "            \"updated_count_for_pred\": word_freq[chosen],\n",
        "            \"vocabulary_size\": len(vocabulary)\n",
        "        }\n",
        "\n",
        "    # --- Unseen word branch (existing behavior) ---\n",
        "    else:\n",
        "        prompt = (\n",
        "            f\"Unable to predict the next word for '{last}'.\\n\"\n",
        "            \"If you want to add a predicted next word now, type it and press Enter.\\n\"\n",
        "            \"Otherwise just press Enter to skip: \"\n",
        "        )\n",
        "        user_input = input(prompt).strip()\n",
        "        cand = re.findall(r\"\\w+\", user_input.lower()) if user_input else []\n",
        "        if not cand:\n",
        "            return f\"Unable to predict the next word for '{last}'. No prediction added.\"\n",
        "\n",
        "        pred = cand[0]\n",
        "\n",
        "        # update bigram counts and frequency/vocab/probs\n",
        "        bigram_counts[last][pred] += 1\n",
        "        word_freq[pred] += 1\n",
        "        vocabulary.add(pred)\n",
        "\n",
        "        total_tokens = sum(word_freq.values()) if word_freq else 0\n",
        "        if total_tokens > 0:\n",
        "            probs = {w: f / total_tokens for w, f in word_freq.items()}\n",
        "\n",
        "        return {\n",
        "            \"message\": f\"Added bigram: ('{last}' -> '{pred}')\",\n",
        "            \"top_predictions_for_last\": [w for w, _ in bigram_counts[last].most_common(3)],\n",
        "            \"updated_count_for_pred\": word_freq[pred],\n",
        "            \"vocabulary_size\": len(vocabulary)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "XKFlJFUou2oX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9kakxd1zy3OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_autocorrect_system():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"          SIMPLE AUTOCORRECT SYSTEM\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Vocabulary size: {len(vocabulary)} tokens\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nEnter a word or sentence (or type 'quit' to exit): \").strip()\n",
        "        if not user_input:\n",
        "            print(\"Please enter some text.\")\n",
        "            continue\n",
        "\n",
        "        if user_input.lower() in (\"quit\", \"exit\", \"q\"):\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        print(\"\\nChoose an action:\")\n",
        "        print(\"1 - Check spelling / suggestions\")\n",
        "        print(\"2 - Predict next word\")\n",
        "        choice = input(\"Enter 1, or 2: \").strip()\n",
        "\n",
        "        if choice == \"1\":\n",
        "            result = check_spelling(user_input)\n",
        "            print(f\"\\nInput: {user_input}\")\n",
        "            if result == \"Correct\":\n",
        "                print(\"✓ Correct\")\n",
        "            else:\n",
        "                print(f\"✗ Suggestion: {result}\")\n",
        "\n",
        "        elif choice == \"2\":\n",
        "            # Call the interactive predictor (it may prompt the user internally)\n",
        "            preds = predict_next_word(user_input)\n",
        "            print(f\"\\nInput: {user_input}\")\n",
        "\n",
        "            # If predictor returned a simple list of suggestions\n",
        "            if isinstance(preds, list):\n",
        "                if preds:\n",
        "                    print(\"Next word predictions:\", \", \".join(preds))\n",
        "                else:\n",
        "                    print(\"No predictions available.\")\n",
        "\n",
        "            # If predictor returned a dict (confirmation / addition info)\n",
        "            elif isinstance(preds, dict):\n",
        "                # Print the clear message and useful stats\n",
        "                msg = preds.get(\"message\", \"Prediction updated.\")\n",
        "                top = preds.get(\"top_predictions_for_last\")\n",
        "                cnt = preds.get(\"updated_count_for_pred\")\n",
        "                vocab_sz = preds.get(\"vocabulary_size\")\n",
        "                print(msg)\n",
        "                if top:\n",
        "                    print(\"Top predictions for the last word now:\", \", \".join(top))\n",
        "                if cnt is not None:\n",
        "                    print(\"Updated count for that prediction:\", cnt)\n",
        "                if vocab_sz is not None:\n",
        "                    print(\"Updated vocabulary size:\", vocab_sz)\n",
        "\n",
        "            # If predictor returned a string (e.g., error / informative message)\n",
        "            elif isinstance(preds, str):\n",
        "                print(preds)\n",
        "\n",
        "            else:\n",
        "                # Fallback for any unexpected return type\n",
        "                print(\"Next word prediction:\", preds)\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid choice. Try again.\")\n",
        "\n",
        "        cont = input(\"\\nContinue? (y/n): \").strip().lower()\n",
        "        if cont in (\"n\", \"no\"):\n",
        "            print(\"Goodbye!\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "oM0x69Kzy2-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Allow running as script\n",
        "if __name__ == \"__main__\":\n",
        "    run_autocorrect_system()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysXh7rDAy_dH",
        "outputId": "cf0540a8-4b4e-4a2d-9c27-b48aaeacd26d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          SIMPLE AUTOCORRECT SYSTEM\n",
            "============================================================\n",
            "Vocabulary size: 132694 tokens\n",
            "------------------------------------------------------------\n",
            "\n",
            "Enter a word or sentence (or type 'quit' to exit): dd\n",
            "\n",
            "Choose an action:\n",
            "1 - Check spelling / suggestions\n",
            "2 - Predict next word\n",
            "Enter 1, or 2: 8\n",
            "Invalid choice. Try again.\n",
            "\n",
            "Continue? (y/n): n\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IDROZTxUy9B1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}